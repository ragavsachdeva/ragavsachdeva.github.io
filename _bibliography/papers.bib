---
---

@misc{sachdeva2024manga,
      abbr={arXiv},
      title={The Manga Whisperer: Automatically Generating Transcriptions for Comics}, 
      author={Ragav Sachdeva and Andrew Zisserman},
      year={2024},
      eprint={2401.10224},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      code={https://github.com/ragavsachdeva/magi},
      arxiv={2401.10224},
      booktitle={ArXiv preprint},
      abstract={In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of diarisation i.e. generating a transcription of who said what and when, in a fully automatic way. To this end, we make the following contributions: (1) we present a unified model, Magi, that is able to (a) detect panels, text boxes and character boxes, (b) cluster characters by identity (without knowing the number of clusters apriori), and (c) associate dialogues to their speakers; (2) we propose a novel approach that is able to sort the detected text boxes in their reading order and generate a dialogue transcript; (3) we annotate an evaluation benchmark for this task using publicly available [English] manga pages.}
}

@inproceedings{sachdeva2023iccvw,
      abbr={ICCVW'23},
      title={The Change You Want to See (Now in 3D)},
      author={Sachdeva, Ragav and Zisserman, Andrew},
      booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
      year={2023},
      arxiv={2308.10417},
      code={https://github.com/ragavsachdeva/CYWS-3D},
      abstract={The goal of this paper is to detect what has changed, if anything, between two "in the wild" images of the same 3D scene acquired from different camera positions and at different temporal instances. The open-set nature of this problem, occlusions/dis-occlusions due to the shift in viewpoint, and the lack of suitable training datasets, presents substantial challenges in devising a solution. To address this problem, we contribute a change detection model that is trained entirely on synthetic data and is class-agnostic, yet it is performant out-of-the-box on real world images without requiring fine-tuning. Our solution entails a "register and difference" approach that leverages self-supervised frozen embeddings and feature differences, which allows the model to generalise to a wide variety of scenes and domains. The model is able to operate directly on two RGB images, without requiring access to ground truth camera intrinsics, extrinsics, depth maps, point clouds, or additional before-after images. Finally, we collect and release a new evaluation dataset consisting of real-world image pairs with human-annotated differences and demonstrate the efficacy of our method.}
}


@InProceedings{sachdeva2023wacv,
      abbr={WACV'23},
      author    = {Sachdeva, Ragav and Zisserman, Andrew},
      title     = {The Change You Want to See},
      booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
      month     = {January},
      year      = {2023},
      arxiv={2209.14341},
      code={https://github.com/ragavsachdeva/The-Change-You-Want-to-See},
      website={https://www.robots.ox.ac.uk/%7Evgg/research/cyws/},
      abstract={We live in a dynamic world where things change all the time. Given two images of the same scene, being able to automatically detect the changes in them has practical applications in a variety of domains. In this paper, we tackle the change detection problem with the goal of detecting "object-level" changes in an image pair despite differences in their viewpoint and illumination. To this end, we make the following four contributions: (i) we propose a scalable methodology for obtaining a large-scale change detection training dataset by leveraging existing object segmentation benchmarks; (ii) we introduce a co-attention based novel architecture that is able to implicitly determine correspondences between an image pair and find changes in the form of bounding box predictions; (iii) we contribute four evaluation datasets that cover a variety of domains and transformations, including synthetic image changes, real surveillance images of a 3D scene, and synthetic 3D scenes with camera motion; (iv) we evaluate our model on these four datasets and demonstrate zero-shot and beyond training transformation generalization.}
}

@article{sachdeva2023pr,
      abbr={PR'23},
      title = {ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning},
      journal = {Pattern Recognition},
      author = {Ragav Sachdeva and Filipe R. Cordeiro and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
      year={2023},
      arxiv={2103.11395},
      code={https://github.com/ragavsachdeva/ScanMix},
      abstract = {We propose a new training algorithm, ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results, and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.}
}

@article{cordeiro2023pr,
      abbr={PR'23},
      title = {LongReMix: Robust learning with high confidence samples in a noisy label environment},
      journal = {Pattern Recognition},
      volume = {133},
      pages = {109013},
      year = {2023},
      issn = {0031-3203},
      doi = {https://doi.org/10.1016/j.patcog.2022.109013},
      url = {https://www.sciencedirect.com/science/article/pii/S0031320322004939},
      author = {Filipe R. Cordeiro and Ragav Sachdeva and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
      keywords = {Noisy label learning, Deep learning, Empirical vicinal risk, Semi-supervised learning},
      abstract = {State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets.},
      arxiv={2103.04173},
      code={https://github.com/filipe-research/LongReMix},
}

@InProceedings{sachdeva2021icra,
      abbr={ICRA'22},
      title={Autonomy and Perception for Space Mining}, 
      author={Ragav Sachdeva and Ravi Hammond and James Bockman and Alec Arthur and Brandon Smart and Dustin Craggs and Anh-Dzung Doan and Thomas Rowntree and Elijah Schutz and Adrian Orenstein and Andy Yu and Tat-Jun Chin and Ian Reid},
      booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
      month={May},
      year={2022},
      arxiv={2109.12109},
      demo={https://www.youtube.com/watch?v=3vYtedQTVbw},
      abstract = {Future Moon bases will likely be constructed using resources mined from the surface of the Moon. The difficulty of maintaining a human workforce on the Moon and communications lag with Earth means that mining will need to be conducted using collaborative robots with a high degree of autonomy. In this paper, we describe our solution for Phase 2 of the NASA Space Robotics Challenge, which provided a simulated lunar environment in which teams were tasked to develop software systems to achieve autonomous collaborative robots for mining on the Moon. Our 3rd place and innovation award winning solution shows how machine learning-enabled vision could alleviate major challenges posed by the lunar environment towards autonomous space mining, chiefly the lack of satellite positioning systems, hazardous terrain, and delicate robot interactions. A robust multi-robot coordinator was also developed to achieve long-term operation and effective collaboration between robots.}
}

@InProceedings{sachdeva2020wacv,
      abbr={WACV'21},
      author    = {Sachdeva, Ragav and Cordeiro, Filipe R. and Belagiannis, Vasileios and Reid, Ian and Carneiro, Gustavo},
      title     = {EvidentialMix: Learning With Combined Open-Set and Closed-Set Noisy Labels},
      booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
      month     = {January},
      year      = {2021},
      pages     = {3607-3615},
      arxiv={2011.05704},
      code={https://github.com/ragavsachdeva/EvidentialMix},
      blog={2020/evidentialmix},
      abstract = {The efficacy of deep learning depends on large-scale data sets that have been carefully curated with reliable data acquisition and annotation processes. However, acquiring such large-scale data sets with precise annotations is very expensive and time-consuming, and the cheap alternatives often yield data sets that have noisy labels. The field has addressed this problem by focusing on training models under two types of label noise: 1) closed-set noise, where some training samples are incorrectly annotated to a training label other than their known true class; and 2) open-set noise, where the training set includes samples that possess a true class that is (strictly) not contained in the set of known training labels. In this work, we study a new variant of the noisy label problem that combines the open-set and closed-set noisy labels, and introduce a benchmark evaluation to assess the performance of training algorithms under this setup. We argue that such problem is more general and better reflects the noisy label scenarios in practice. Furthermore, we propose a novel algorithm, called EvidentialMix, that addresses this problem and compare its performance with the state-of-the-art methods for both closed-set and open-set noise on the proposed benchmark. Our results show that our method produces superior classification results and better feature representations than previous state-of-the-art methods.}
}